{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "240654965e339d128a9bbd5be5d7766dbc7de156d15b1c12ed426f21518e5300"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 再次使用Gluon来实现一个softmax回归模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "\n",
    "%matplotlib inline\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet import autograd, nd\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据\n",
    "mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "# 测试数据\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False)\n",
    "\n",
    "def load_data_fashion_mnist(batch_size):\n",
    "    transformer = gdata.vision.transforms.ToTensor()\n",
    "    if sys.platform.startswith('win'):\n",
    "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "    else:\n",
    "        num_workers = 4\n",
    "\n",
    "    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n",
    "                                batch_size, shuffle=True,\n",
    "                                num_workers=num_workers)\n",
    "    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n",
    "                                batch_size, shuffle=False,\n",
    "                                num_workers=num_workers)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# 读取数据\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义和初始化模型\n",
    "# softmax回归的输出层是一个全连接层。\n",
    "# 因此，我们添加一个输出个数为10的全连接层。\n",
    "# 使用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数。\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax和交叉熵损失函数\n",
    "# Gluon提供了一个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化算法\n",
    "# 'sgd' 小批量随机梯度下降\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss 0.7897, train acc 0.746, test acc 0.806\n",
      "epoch 2, loss 0.5738, train acc 0.811, test acc 0.823\n",
      "epoch 3, loss 0.5296, train acc 0.824, test acc 0.826\n",
      "epoch 4, loss 0.5057, train acc 0.829, test acc 0.834\n",
      "epoch 5, loss 0.4898, train acc 0.835, test acc 0.837\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 5\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "source": [
    "## 总结\n",
    "\n",
    "- Gluon提供的函数往往具有更好的数值稳定性。\n",
    "\n",
    "- 可以使用Gluon更简洁地实现softmax回归。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}