# 入门

- 强化学习是相对监督学习而言的

训练数据没有标签，但是有一个奖励函数

训练数据不是现成的，而是由行为 action 获取

当前行为影响：奖励函数值、下一个训练数据

学习目的：家里一个 状态->行为 映射关系，即什么情况下应采取什么行为

举例：下棋、无人驾驶

- 符号和概念

Rt t时刻的奖励函数值

St t时刻的状态

At t时刻的行为

P(A|S, R) S/R确定下采取A行为的概率

π(S,a) 模型在S状态下，采取a行为的概率

最大化累积奖励 学习的目标

估值函数Vπ(s) 训练好的模型，考虑所有环境概率下，能获得多少累计奖励。即 ΣP(情况i)*情况i下能得到的累计奖励。下棋游戏就是这个人的胜率

Q函数 训练好的模型，某情况某时刻，采取A行为后，预估最终能获得多少累计奖励

```
估值函数 = ΣP(A)Q(A)
```

```
估值函数和Q函数，可以使用神经网络计算给出
估值网络
```

- 假设

马尔科夫假设：P(S(t+1)|S(t)) = P(S(t+1)|S(t),S(t-1),S(t-1)..S(0))

下一个时刻的状态，只与当前状态和当前行为有关

下一个时刻的奖励函数值，只与这一时刻的状态和行为有关

```
简单理解：
现在 => 未来 √
过去 + 现在 => 未来 X
```

- 过程描述

t=0时，给出一个初始状态 S(0)
for t 0:end
  模型选择行为 A(t)
  得到奖励 R(t) = R(S(t), A(t))
  产生下一个状态 S(t+1)

- 目标：训练模型 P(A|S, R)，最大化累积奖励 Σ(γ^t)*R(t)

γ<1，更看重前面行为的奖励

- 随机行为，跳出固定思维模式

- 记忆内存

- 演员批评家算法

